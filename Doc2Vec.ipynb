{
 "metadata": {
  "name": "",
  "signature": "sha256:17b6ebce29f510d63285226fc4ed03fac7a49eddfd855b3d7082c0ff2014153b"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "import data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "\n",
      "# Read data from files \n",
      "train = pd.read_csv( \"labeledTrainData.tsv\", header=0, \n",
      " delimiter=\"\\t\", quoting=3 )\n",
      "test = pd.read_csv( \"testData.tsv\", header=0, delimiter=\"\\t\", quoting=3 )\n",
      "unlabeled_train = pd.read_csv( \"unlabeledTrainData.tsv\", header=0, \n",
      " delimiter=\"\\t\", quoting=3 )\n",
      "\n",
      "# Verify the number of reviews that were read (100,000 in total)\n",
      "print \"Read %d labeled train reviews, %d labeled test reviews, \" \\\n",
      " \"and %d unlabeled reviews\\n\" % (train[\"review\"].size,  \n",
      " test[\"review\"].size, unlabeled_train[\"review\"].size )\n",
      "print train.shape, test.shape, unlabeled_train.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Read 25000 labeled train reviews, 25000 labeled test reviews, and 50000 unlabeled reviews\n",
        "\n",
        "(25000, 3) (25000, 2) (50000, 2)\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "define functions to generate words and sentences lists"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Import various modules for string cleaning\n",
      "from bs4 import BeautifulSoup\n",
      "import re\n",
      "from nltk.corpus import stopwords\n",
      "\n",
      "def review_to_wordlist( review, remove_stopwords=False ):\n",
      "    # Function to convert a document to a sequence of words,\n",
      "    # optionally removing stop words.  Returns a list of words.\n",
      "    #\n",
      "    # 1. Remove HTML\n",
      "    review_text = BeautifulSoup(review).get_text()\n",
      "    #  \n",
      "    # 2. Remove non-letters\n",
      "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
      "    #\n",
      "    # 3. Convert words to lower case and split them\n",
      "    words = review_text.lower().split()\n",
      "    #\n",
      "    # 4. Optionally remove stop words (false by default)\n",
      "    if remove_stopwords:\n",
      "        stops = set(stopwords.words(\"english\"))\n",
      "        words = [w for w in words if not w in stops]\n",
      "    #\n",
      "    # 5. Return a list of words\n",
      "    return(words)\n",
      "\n",
      "# Download the punkt tokenizer for sentence splitting\n",
      "import nltk.data\n",
      "#nltk.download()   \n",
      "\n",
      "# Load the punkt tokenizer\n",
      "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
      "\n",
      "# Define a function to split a review into parsed sentences\n",
      "def review_to_sentences( review, tokenizer, remove_stopwords=False ):\n",
      "    # Function to split a review into parsed sentences. Returns a \n",
      "    # list of sentences, where each sentence is a list of words\n",
      "    #\n",
      "    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
      "    raw_sentences = tokenizer.tokenize(review.decode('utf-8').strip())\n",
      "    #\n",
      "    # 2. Loop over each sentence\n",
      "    sentences = []\n",
      "    for raw_sentence in raw_sentences:\n",
      "        # If a sentence is empty, skip it\n",
      "        if len(raw_sentence) > 0:\n",
      "            # Otherwise, call review_to_wordlist to get a list of words\n",
      "            sentences.append( review_to_wordlist( raw_sentence, remove_stopwords ))\n",
      "    #\n",
      "    # Return the list of sentences (each sentence is a list of words,\n",
      "    # so this returns a list of lists\n",
      "    return sentences"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "processing training data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sentList = []  # train + unlabeled_train\n",
      "allSentList = [] # train + test + unlabeled_train\n",
      "sentNumPerReview_train = [] # Count the number of sentences in each review of labeled training data\n",
      "sentNumPerReview_unlabeled_train = [] # Count the number of sentences in each review of unlabeled training data\n",
      "sentNumPerReview_test = [] # Count the number of sentences in each review of test data\n",
      "\n",
      "print \"Parsing sentences from training set\"\n",
      "for review in train[\"review\"]:\n",
      "    sent = review_to_sentences(review, tokenizer)\n",
      "    sentNumPerReview_train.append(len(sent))\n",
      "    sentList += sent\n",
      "\n",
      "print \"Parsing sentences from unlabeled set\"\n",
      "for review in unlabeled_train[\"review\"]:\n",
      "    sent = review_to_sentences(review, tokenizer)\n",
      "    sentNumPerReview_unlabeled_train.append(len(sent))\n",
      "    sentList += sent\n",
      "\n",
      "print \"Parsing sentences from test set\"\n",
      "allSentList = sentList\n",
      "for review in test[\"review\"]:\n",
      "    sent = review_to_sentences(review, tokenizer)\n",
      "    sentNumPerReview_test.append(len(sent))\n",
      "    allSentList += sent\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Parsing sentences from training set\n",
        "Parsing sentences from unlabeled set"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Parsing sentences from test set"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "create a word2vec model"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Import the built-in logging module and configure it so that Word2Vec \n",
      "# creates nice output messages\n",
      "import logging\n",
      "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
      "    level=logging.INFO)\n",
      "\n",
      "# Set values for various parameters\n",
      "num_features = 100    # Word vector dimensionality                      \n",
      "min_word_count = 40   # Minimum word count                        \n",
      "num_workers = 4       # Number of threads to run in parallel\n",
      "context = 10          # Context window size                                                                                    \n",
      "downsampling = 1e-3   # Downsample setting for frequent words\n",
      "\n",
      "# Initialize and train the model (this will take some time)\n",
      "from gensim.models import word2vec\n",
      "print \"Training model...\"\n",
      "model_w2v = word2vec.Word2Vec(sentList, workers=num_workers, \\\n",
      "            size=num_features, min_count = min_word_count, \\\n",
      "            window = context, sample = downsampling)\n",
      "\n",
      "# If you don't plan to train the model any further, calling \n",
      "# init_sims will make the model much more memory-efficient.\n",
      "model_w2v.init_sims(replace=True)\n",
      "\n",
      "# It can be helpful to create a meaningful model name and \n",
      "# save the model for later use. You can load it later using Word2Vec.load()\n",
      "model_name = \"300features_40minwords_10context_word2vec\"\n",
      "model_w2v.save(model_name)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Training model...\n"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "average word feature vectors"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np  # Make sure that numpy is imported\n",
      "\n",
      "def makeFeatureVec(words, model, num_features):\n",
      "    # Function to average all of the word vectors in a given\n",
      "    # paragraph\n",
      "    #\n",
      "    # Pre-initialize an empty numpy array (for speed)\n",
      "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
      "    #\n",
      "    nwords = 0\n",
      "    # \n",
      "    # Index2word is a list that contains the names of the words in \n",
      "    # the model's vocabulary. Convert it to a set, for speed \n",
      "    index2word_set = set(model.index2word)\n",
      "    #\n",
      "    # Loop over each word in the review and, if it is in the model's\n",
      "    # vocaublary, add its feature vector to the total\n",
      "    for word in words:\n",
      "        if word in index2word_set: \n",
      "            nwords = nwords + 1\n",
      "            featureVec = np.add(featureVec,model[word])\n",
      "    # \n",
      "    # Divide the result by the number of words to get the average\n",
      "    featureVec = np.divide(featureVec,nwords)\n",
      "    return featureVec\n",
      "\n",
      "\n",
      "def getAvgFeatureVecs(reviews, model, num_features):\n",
      "    # Given a set of reviews (each one a list of words), calculate \n",
      "    # the average feature vector for each one and return a 2D numpy array \n",
      "    # \n",
      "    # Initialize a counter\n",
      "    counter = 0\n",
      "    # \n",
      "    # Preallocate a 2D numpy array, for speed\n",
      "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
      "    # \n",
      "    # Loop through the reviews\n",
      "    for review in reviews:\n",
      "       #\n",
      "       # Print a status message every 1000th review\n",
      "       if counter%1000 == 0:\n",
      "           print \"Review %d of %d\" % (counter, len(reviews))\n",
      "       # \n",
      "       # Call the function (defined above) that makes average feature vectors\n",
      "       reviewFeatureVecs[counter] = makeFeatureVec(review, model, \\\n",
      "           num_features)\n",
      "       #\n",
      "       # Increment the counter\n",
      "       counter = counter + 1\n",
      "    return reviewFeatureVecs\n",
      "\n",
      "# ****************************************************************\n",
      "# Calculate average feature vectors for training and testing sets,\n",
      "# using the functions we defined above. Notice that we now use stop word\n",
      "# removal.\n",
      "\n",
      "clean_train_reviews = []\n",
      "for review in train[\"review\"]:\n",
      "    clean_train_reviews.append( review_to_wordlist( review, \\\n",
      "        remove_stopwords=True ))\n",
      "\n",
      "trainDataVecs = getAvgFeatureVecs( clean_train_reviews, model_w2v, num_features )\n",
      "\n",
      "print \"Creating average feature vecs for test reviews\"\n",
      "clean_test_reviews = []\n",
      "for review in test[\"review\"]:\n",
      "    clean_test_reviews.append( review_to_wordlist( review, \\\n",
      "        remove_stopwords=True ))\n",
      "\n",
      "testDataVecs = getAvgFeatureVecs( clean_test_reviews, model_w2v, num_features )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Review 0 of 25000\n",
        "Review 1000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 2000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 3000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 4000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 5000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 6000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 7000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 8000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 9000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 10000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 11000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 12000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 13000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 14000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 15000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 16000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 17000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 18000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 19000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 20000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 21000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 22000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 23000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 24000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Creating average feature vecs for test reviews"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 0 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 1000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 2000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 3000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 4000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 5000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 6000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 7000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 8000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 9000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 10000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 11000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 12000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 13000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 14000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 15000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 16000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 17000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 18000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 19000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 20000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 21000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 22000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 23000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 24000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "convert review list to LabeledSentence object"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from gensim.models import doc2vec\n",
      "# create LabeledSentece object, note the difference between 'label' here and in 'labeled training data' context.\n",
      "labeledSentList = [];\n",
      "sentID = 0;\n",
      "for sent in allSentList:\n",
      "    sentID += 1\n",
      "    labeledSent = doc2vec.LabeledSentence(words=allSentList[sentID-1],labels=['SENT_%s' % sentID])\n",
      "    labeledSentList.append(labeledSent)\n",
      "num_doc_features = 100\n",
      "window_size = 4  # 4 is optimum\n",
      "model_d2v = doc2vec.Doc2Vec(labeledSentList, size=num_doc_features, window=window_size, min_count=2, workers=4)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "average paragraph feature vectors"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np  # Make sure that numpy is imported\n",
      "\n",
      "# ****************************************************************\n",
      "# Calculate average feature vectors for training and testing sets,\n",
      "# using the functions we defined above. Notice that we now use stop word\n",
      "# removal.\n",
      "\n",
      "clean_train_reviews = []\n",
      "for review in train[\"review\"]:\n",
      "    clean_train_reviews.append( review_to_wordlist( review, \\\n",
      "        remove_stopwords=True ))\n",
      "    \n",
      "trainDocFeatureVec = np.zeros((len(clean_train_reviews), num_doc_features),dtype=\"float32\")\n",
      "idx = 0\n",
      "for i in xrange(len(clean_train_reviews)):\n",
      "    count = 0\n",
      "    for j in xrange(sentNumPerReview_train[i]): \n",
      "        idx += 1\n",
      "        if len(labeledSentList[idx-1].words) >= window_size:\n",
      "            count += 1\n",
      "            trainDocFeatureVec[i] = np.add(trainDocFeatureVec[i], model_d2v['SENT_%s' % idx])\n",
      "    if count != 0:\n",
      "        trainDocFeatureVec[i] = np.divide(trainDocFeatureVec[i],count)\n",
      "\n",
      "print \"Creating average feature vecs for test reviews\"\n",
      "clean_test_reviews = []\n",
      "for review in test[\"review\"]:\n",
      "    clean_test_reviews.append( review_to_wordlist( review, \\\n",
      "        remove_stopwords=True ))\n",
      "\n",
      "testDocFeatureVec = np.zeros((len(clean_test_reviews), num_doc_features),dtype=\"float32\")\n",
      "for i in xrange(len(clean_test_reviews)):\n",
      "    count = 0\n",
      "    for j in xrange(sentNumPerReview_test[i]): \n",
      "        idx += 1\n",
      "        if len(labeledSentList[idx-1].words) >= window_size:\n",
      "            count += 1\n",
      "            testDocFeatureVec[i] = np.add(testDocFeatureVec[i], model_d2v['SENT_%s' % idx])\n",
      "    if count != 0:\n",
      "        testDocFeatureVec[i] = np.divide(testDocFeatureVec[i],count)\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Creating average feature vecs for test reviews\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "concatenate word and paragraph feature vectors"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "trainDataVecs = np.concatenate((trainDocFeatureVec,trainDataVecs), axis=1)\n",
      "testDataVecs = np.concatenate((testDocFeatureVec,testDataVecs), axis=1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "predict with random forest"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Fit a random forest to the training data, using 100 trees\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "forest = RandomForestClassifier( n_estimators = 300 )\n",
      "\n",
      "print \"Fitting a random forest to labeled training data...\"\n",
      "forest = forest.fit( trainDataVecs, train[\"sentiment\"] )\n",
      "\n",
      "# Test & extract results \n",
      "result = forest.predict( testDataVecs )\n",
      "\n",
      "# Write the test results \n",
      "output = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":result} )\n",
      "output.to_csv( \"Word2Vec_AverageVectors.csv\", index=False, quoting=3 )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Fitting a random forest to labeled training data...\n"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "trainDocFeatureVec.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 24,
       "text": [
        "(25000, 100)"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "trainDataVecs.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 25,
       "text": [
        "(25000, 200)"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x=np.concatenate((trainDocFeatureVec,trainDataVecs), axis=1)\n",
      "x.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 26,
       "text": [
        "(25000, 300)"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train['review'][2]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 27,
       "text": [
        "'\"The film starts with a manager (Nicholas Bell) giving welcome investors (Robert Carradine) to Primal Park . A secret project mutating a primal animal using fossilized DNA, like \\xc2\\xa8Jurassik Park\\xc2\\xa8, and some scientists resurrect one of nature\\'s most fearsome predators, the Sabretooth tiger or Smilodon . Scientific ambition turns deadly, however, and when the high voltage fence is opened the creature escape and begins savagely stalking its prey - the human visitors , tourists and scientific.Meanwhile some youngsters enter in the restricted area of the security center and are attacked by a pack of large pre-historical animals which are deadlier and bigger . In addition , a security agent (Stacy Haiduk) and her mate (Brian Wimmer) fight hardly against the carnivorous Smilodons. The Sabretooths, themselves , of course, are the real star stars and they are astounding terrifyingly though not convincing. The giant animals savagely are stalking its prey and the group run afoul and fight against one nature\\'s most fearsome predators. Furthermore a third Sabretooth more dangerous and slow stalks its victims.<br /><br />The movie delivers the goods with lots of blood and gore as beheading, hair-raising chills,full of scares when the Sabretooths appear with mediocre special effects.The story provides exciting and stirring entertainment but it results to be quite boring .The giant animals are majority made by computer generator and seem totally lousy .Middling performances though the players reacting appropriately to becoming food.Actors give vigorously physical performances dodging the beasts ,running,bound and leaps or dangling over walls . And it packs a ridiculous final deadly scene. No for small kids by realistic,gory and violent attack scenes . Other films about Sabretooths or Smilodon are the following : \\xc2\\xa8Sabretooth(2002)\\xc2\\xa8by James R Hickox with Vanessa Angel, David Keith and John Rhys Davies and the much better \\xc2\\xa810.000 BC(2006)\\xc2\\xa8 by Roland Emmerich with with Steven Strait, Cliff Curtis and Camilla Belle. This motion picture filled with bloody moments is badly directed by George Miller and with no originality because takes too many elements from previous films. Miller is an Australian director usually working for television (Tidal wave, Journey to the center of the earth, and many others) and occasionally for cinema ( The man from Snowy river, Zeus and Roxanne,Robinson Crusoe ). Rating : Below average, bottom of barrel.\"'"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>id</th>\n",
        "      <th>sentiment</th>\n",
        "      <th>review</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td> \"5814_8\"</td>\n",
        "      <td> 1</td>\n",
        "      <td> \"With all this stuff going down at the moment ...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td> \"2381_9\"</td>\n",
        "      <td> 1</td>\n",
        "      <td> \"\\\"The Classic War of the Worlds\\\" by Timothy ...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td> \"7759_3\"</td>\n",
        "      <td> 0</td>\n",
        "      <td> \"The film starts with a manager (Nicholas Bell...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td> \"3630_4\"</td>\n",
        "      <td> 0</td>\n",
        "      <td> \"It must be assumed that those who praised thi...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td> \"9495_8\"</td>\n",
        "      <td> 1</td>\n",
        "      <td> \"Superbly trashy and wondrously unpretentious ...</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 28,
       "text": [
        "         id  sentiment                                             review\n",
        "0  \"5814_8\"          1  \"With all this stuff going down at the moment ...\n",
        "1  \"2381_9\"          1  \"\\\"The Classic War of the Worlds\\\" by Timothy ...\n",
        "2  \"7759_3\"          0  \"The film starts with a manager (Nicholas Bell...\n",
        "3  \"3630_4\"          0  \"It must be assumed that those who praised thi...\n",
        "4  \"9495_8\"          1  \"Superbly trashy and wondrously unpretentious ..."
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 28
    }
   ],
   "metadata": {}
  }
 ]
}