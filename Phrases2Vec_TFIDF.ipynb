{
 "metadata": {
  "name": "",
  "signature": "sha256:fc4d7cca3981457090f7a98b0fb3cbb3d434b2f0784076bb42b60466fb96b2b5"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import numpy as np\n",
      "train = pd.read_csv('labeledTrainData.tsv', header=0, delimiter=\"\\t\", quoting=3)\n",
      "unlabeled_train = pd.read_csv('unlabeledTrainData.tsv', header=0, delimiter=\"\\t\", quoting=3)\n",
      "test = pd.read_csv('testData.tsv', header=0, delimiter=\"\\t\", quoting=3 )\n",
      "y = train[\"sentiment\"]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "calcualte tfidf score"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "from KaggleWord2VecUtility import KaggleWord2VecUtility\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn import cross_validation\n",
      "print \"Cleaning and parsing movie reviews...\\n\"\n",
      "traindata = []\n",
      "for i in xrange( 0, len(train[\"review\"])):\n",
      "    traindata.append(\" \".join(KaggleWord2VecUtility.review_to_wordlist(train[\"review\"][i], True)))\n",
      "testdata = []\n",
      "for i in xrange(0,len(test[\"review\"])):\n",
      "    testdata.append(\" \".join(KaggleWord2VecUtility.review_to_wordlist(test[\"review\"][i], True)))\n",
      "unlabeled_traindata = []\n",
      "for i in xrange(0,len(unlabeled_train[\"review\"])):\n",
      "    unlabeled_traindata.append(\" \".join(KaggleWord2VecUtility.review_to_wordlist(unlabeled_train[\"review\"][i], True)))\n",
      "print 'vectorizing... ' \n",
      "tfv = TfidfVectorizer(min_df=1,  max_features=None, \n",
      "        strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
      "        ngram_range=(1, 1), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
      "        stop_words = 'english')\n",
      "alldata = traindata + testdata + unlabeled_traindata\n",
      "lentrain = len(traindata)\n",
      "lentest = len(testdata)\n",
      "print \"fitting pipeline... \"\n",
      "tfv.fit(alldata)\n",
      "alldata = tfv.transform(alldata)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Cleaning and parsing movie reviews...\n",
        "\n",
        "vectorizing... "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "fitting pipeline... \n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(tfv.get_feature_names())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "141055"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "import and prepare data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Import various modules for string cleaning\n",
      "from bs4 import BeautifulSoup\n",
      "import re\n",
      "from nltk.corpus import stopwords\n",
      "\n",
      "def review_to_wordlist( review, remove_stopwords=False ):\n",
      "    # Function to convert a document to a sequence of words,\n",
      "    # optionally removing stop words.  Returns a list of words.\n",
      "    #\n",
      "    # 1. Remove HTML\n",
      "    review_text = BeautifulSoup(review).get_text()\n",
      "    #  \n",
      "    # 2. Remove non-letters\n",
      "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
      "    #\n",
      "    # 3. Convert words to lower case and split them\n",
      "    words = review_text.lower().split()\n",
      "    #\n",
      "    # 4. Optionally remove stop words (false by default)\n",
      "    if remove_stopwords:\n",
      "        stops = set(stopwords.words(\"english\"))\n",
      "        words = [w for w in words if not w in stops]\n",
      "    #\n",
      "    # 5. Return a list of words\n",
      "    return(words)\n",
      "\n",
      "# Download the punkt tokenizer for sentence splitting\n",
      "import nltk.data\n",
      "#nltk.download()   \n",
      "\n",
      "# Load the punkt tokenizer\n",
      "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
      "\n",
      "# Define a function to split a review into parsed sentences\n",
      "def review_to_sentences( review, tokenizer, remove_stopwords=False ):\n",
      "    # Function to split a review into parsed sentences. Returns a \n",
      "    # list of sentences, where each sentence is a list of words\n",
      "    #\n",
      "    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
      "    raw_sentences = tokenizer.tokenize(review.decode('utf-8').strip())\n",
      "    #\n",
      "    # 2. Loop over each sentence\n",
      "    sentences = []\n",
      "    for raw_sentence in raw_sentences:\n",
      "        # If a sentence is empty, skip it\n",
      "        if len(raw_sentence) > 0:\n",
      "            # Otherwise, call review_to_wordlist to get a list of words\n",
      "            sentences.append( review_to_wordlist( raw_sentence, remove_stopwords ))\n",
      "    #\n",
      "    # Return the list of sentences (each sentence is a list of words,\n",
      "    # so this returns a list of lists\n",
      "    return sentences\n",
      "\n",
      "\n",
      "reviewList = []  # Initialize an empty list of sentences\n",
      "trainSentNumList = []\n",
      "unlabeledTrainSentNumList = []\n",
      "testSentNumList = []\n",
      "print \"Parsing sentences from training set\"\n",
      "for review in train[\"review\"]:\n",
      "    sents = review_to_sentences(review, tokenizer, True)\n",
      "    reviewList += sents\n",
      "    trainSentNumList.append(len(sents))\n",
      "\n",
      "print \"Parsing sentences from test set\"\n",
      "for review in test[\"review\"]:\n",
      "    sents = review_to_sentences(review, tokenizer, True)\n",
      "    reviewList += sents\n",
      "    testSentNumList.append(len(sents))\n",
      "\n",
      "print \"Parsing sentences from unlabeled training set\"\n",
      "for review in unlabeled_train[\"review\"]:\n",
      "    sents = review_to_sentences(review, tokenizer, True)\n",
      "    reviewList += sents\n",
      "    unlabeledTrainSentNumList.append(len(sents))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/Users/yunfengxi/anaconda/lib/python2.7/site-packages/bs4/__init__.py:169: UserWarning: \".\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
        "  '\"%s\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.' % markup)\n",
        "/Users/yunfengxi/anaconda/lib/python2.7/site-packages/bs4/__init__.py:176: UserWarning: \"http://www.happierabroad.com\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
        "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Parsing sentences from training set\n",
        "Parsing sentences from test set"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Parsing sentences from unlabeled training set"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/Users/yunfengxi/anaconda/lib/python2.7/site-packages/bs4/__init__.py:169: UserWarning: \"..\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
        "  '\"%s\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.' % markup)\n",
        "/Users/yunfengxi/anaconda/lib/python2.7/site-packages/bs4/__init__.py:176: UserWarning: \"http://www.archive.org/details/LovefromaStranger\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
        "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/Users/yunfengxi/anaconda/lib/python2.7/site-packages/bs4/__init__.py:176: UserWarning: \"http://www.loosechangeguide.com/LooseChangeGuide.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
        "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
        "/Users/yunfengxi/anaconda/lib/python2.7/site-packages/bs4/__init__.py:176: UserWarning: \"http://www.msnbc.msn.com/id/4972055/site/newsweek/\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
        "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/Users/yunfengxi/anaconda/lib/python2.7/site-packages/bs4/__init__.py:176: UserWarning: \"http://www.youtube.com/watch?v=a0KSqelmgN8\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
        "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
        "/Users/yunfengxi/anaconda/lib/python2.7/site-packages/bs4/__init__.py:176: UserWarning: \"http://jake-weird.blogspot.com/2007/08/beneath.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
        "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "generate multi-gram phrases"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from gensim.models import phrases\n",
      "bigram = phrases.Phrases(sentences=reviewList, min_count=5, threshold=25.0, max_vocab_size=40000000, delimiter=' ')\n",
      "# generate bigram\n",
      "all_bigramList = []\n",
      "for sent in reviewList:\n",
      "    all_bigramList.append(bigram[sent])\n",
      "\n",
      "trigram = phrases.Phrases(sentences=all_bigramList, min_count=2, threshold=2.0, max_vocab_size=40000000, delimiter=' ')\n",
      "# generate trigram\n",
      "all_trigramList = []\n",
      "for sent in all_bigramList:\n",
      "    all_trigramList.append(trigram[sent])\n",
      "    \n",
      "data_to_use = all_bigramList"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/Users/yunfengxi/anaconda/lib/python2.7/site-packages/numpy/lib/utils.py:95: DeprecationWarning: `scipy.sparse.sparsetools` is deprecated!\n",
        "scipy.sparse.sparsetools is a private module for scipy.sparse, and should not be used.\n",
        "  warnings.warn(depdoc, DeprecationWarning)\n",
        "/Users/yunfengxi/anaconda/lib/python2.7/site-packages/scipy/lib/_util.py:67: DeprecationWarning: Module scipy.linalg.blas.fblas is deprecated, use scipy.linalg.blas instead\n",
        "  DeprecationWarning)\n",
        "/Users/yunfengxi/anaconda/lib/python2.7/site-packages/scipy/lib/_util.py:67: DeprecationWarning: Module scipy.linalg.blas.fblas is deprecated, use scipy.linalg.blas instead\n",
        "  DeprecationWarning)\n",
        "/Users/yunfengxi/anaconda/lib/python2.7/site-packages/scipy/lib/_util.py:67: DeprecationWarning: Module scipy.linalg.blas.fblas is deprecated, use scipy.linalg.blas instead\n",
        "  DeprecationWarning)\n",
        "/Users/yunfengxi/anaconda/lib/python2.7/site-packages/scipy/lib/_util.py:67: DeprecationWarning: Module scipy.linalg.blas.fblas is deprecated, use scipy.linalg.blas instead\n",
        "  DeprecationWarning)\n",
        "/Users/yunfengxi/anaconda/lib/python2.7/site-packages/scipy/lib/_util.py:67: DeprecationWarning: Module scipy.linalg.blas.fblas is deprecated, use scipy.linalg.blas instead\n",
        "  DeprecationWarning)\n",
        "/Users/yunfengxi/anaconda/lib/python2.7/site-packages/scipy/lib/_util.py:67: DeprecationWarning: Module scipy.linalg.blas.fblas is deprecated, use scipy.linalg.blas instead\n",
        "  DeprecationWarning)\n",
        "/Users/yunfengxi/anaconda/lib/python2.7/site-packages/scipy/lib/_util.py:67: DeprecationWarning: Module scipy.linalg.blas.fblas is deprecated, use scipy.linalg.blas instead\n",
        "  DeprecationWarning)\n",
        "/Users/yunfengxi/anaconda/lib/python2.7/site-packages/scipy/lib/_util.py:67: DeprecationWarning: Module scipy.linalg.blas.fblas is deprecated, use scipy.linalg.blas instead\n",
        "  DeprecationWarning)\n",
        "/Users/yunfengxi/anaconda/lib/python2.7/site-packages/scipy/lib/_util.py:67: DeprecationWarning: Module scipy.linalg.blas.fblas is deprecated, use scipy.linalg.blas instead\n",
        "  DeprecationWarning)\n",
        "/Users/yunfengxi/anaconda/lib/python2.7/site-packages/scipy/lib/_util.py:67: DeprecationWarning: Module scipy.linalg.blas.fblas is deprecated, use scipy.linalg.blas instead\n",
        "  DeprecationWarning)\n",
        "/Users/yunfengxi/anaconda/lib/python2.7/site-packages/scipy/lib/_util.py:67: DeprecationWarning: Module scipy.linalg.blas.fblas is deprecated, use scipy.linalg.blas instead\n",
        "  DeprecationWarning)\n",
        "/Users/yunfengxi/anaconda/lib/python2.7/site-packages/scipy/lib/_util.py:67: DeprecationWarning: Module scipy.linalg.blas.fblas is deprecated, use scipy.linalg.blas instead\n",
        "  DeprecationWarning)\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Import the built-in logging module and configure it so that Word2Vec \n",
      "# creates nice output messages\n",
      "import logging\n",
      "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
      "    level=logging.INFO)\n",
      "\n",
      "# Set values for various parameters\n",
      "num_features = 200    # Word vector dimensionality                      \n",
      "min_word_count = 10   # Minimum word count                        \n",
      "num_workers = 4       # Number of threads to run in parallel\n",
      "context = 14          # Context window size                                                                                    \n",
      "downsampling = 1e-4   # Downsample setting for frequent words\n",
      "skip_gram = 1         # 1 is skip gram, 0 is cbow\n",
      "rand_seed = 123456    # random sampling seed\n",
      "hierarc_sampling = 1  # 1 is hierarchical sampling on, 0 is off\n",
      "negative_sampling = 0 # >0 is negative sampling\n",
      "epoch = 20            # number of iteration\n",
      "\n",
      "# Initialize and train the model (this will take some time)\n",
      "from gensim.models import word2vec\n",
      "print \"Training model...\"\n",
      "model = word2vec.Word2Vec(data_to_use, workers = num_workers, \\\n",
      "            size = num_features, min_count = min_word_count, \\\n",
      "            window = context, sample = downsampling, \\\n",
      "            seed = rand_seed, negative = negative_sampling, \\\n",
      "            hs = hierarc_sampling)\n",
      "\n",
      "# If you don't plan to train the model any further, calling \n",
      "# init_sims will make the model much more memory-efficient.\n",
      "model.init_sims(replace=True)\n",
      "\n",
      "# It can be helpful to create a meaningful model name and \n",
      "# save the model for later use. You can load it later using Word2Vec.load()\n",
      "model_name = \"200features_10minwords_14context\"\n",
      "model.save(model_name)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Training model...\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "vectorizing all words"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "trainset = alldata[:lentrain]\n",
      "testset = alldata[lentrain:(lentrain+lentest)]\n",
      "\n",
      "# model = word2vec.Word2Vec.load(\"200features_10minwords_14context\")\n",
      "import numpy as np  # Make sure that numpy is imported\n",
      "tfv_feature_name_list = tfv.get_feature_names()\n",
      "def makeFeatureVec(words, model, num_features, tfv_feature_name_list, tfidfVec):\n",
      "    # Function to average all of the word vectors in a given\n",
      "    # paragraph\n",
      "    #\n",
      "    # Pre-initialize an empty numpy array (for speed)\n",
      "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
      "    #\n",
      "    nwords = 0\n",
      "    # \n",
      "    # Index2word is a list that contains the names of the words in \n",
      "    # the model's vocabulary. Convert it to a set, for speed \n",
      "    index2word_set = set(model.index2word)\n",
      "    #\n",
      "    tfv_feature_name = [tfv_feature_name_list[i] for i in tfidfVec.indices];\n",
      "    #\n",
      "    # Loop over each word in the review and, if it is in the model's\n",
      "    # vocaublary, add its feature vector to the total\n",
      "    weight = 0.0\n",
      "    for i in xrange(len(tfv_feature_name)):\n",
      "        if tfv_feature_name[i] in index2word_set:\n",
      "            weight += tfidfVec.data[i]\n",
      "            featureVec = np.add(featureVec,model[tfv_feature_name[i]]*tfidfVec.data[i])\n",
      "    # \n",
      "    # Divide the result by the number of words to get the average\n",
      "    featureVec = np.divide(featureVec,weight)\n",
      "    return featureVec\n",
      "\n",
      "\n",
      "def getAvgFeatureVecs(reviews, model, num_features, tfidfMat):\n",
      "    # Given a set of reviews (each one a list of words), calculate \n",
      "    # the average feature vector for each one and return a 2D numpy array \n",
      "    # \n",
      "    # Initialize a counter\n",
      "    counter = 0\n",
      "    # \n",
      "    # Preallocate a 2D numpy array, for speed\n",
      "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
      "    # \n",
      "    # Loop through the reviews\n",
      "    for review in reviews:\n",
      "       #\n",
      "       # Print a status message every 1000th review\n",
      "       if counter%1000 == 0:\n",
      "           print \"Review %d of %d\" % (counter, len(reviews))\n",
      "       # \n",
      "       # Call the function (defined above) that makes average feature vectors\n",
      "       reviewFeatureVecs[counter] = makeFeatureVec(review, model, \\\n",
      "           num_features, tfv_feature_name_list, tfidfMat.getrow(counter))\n",
      "       #\n",
      "       # Increment the counter\n",
      "       counter = counter + 1\n",
      "    return reviewFeatureVecs\n",
      "\n",
      "# ****************************************************************\n",
      "# Calculate average feature vectors for training and testing sets,\n",
      "# using the functions we defined above. Notice that we now use stop word\n",
      "# removal.\n",
      "'''\n",
      "import itertools\n",
      "allList = data_to_use  # which gram list are you using, unigram, bigram or trigram?\n",
      "print \"Creating average feature vecs for training reviews\"\n",
      "clean_train_reviews = []\n",
      "count = 0\n",
      "stops = set(stopwords.words(\"english\"))\n",
      "for i in xrange(len(trainSentNumList)):\n",
      "    chain = itertools.chain(*allList[count:count+trainSentNumList[i]])\n",
      "    clean_review = [w for w in list(chain) if not w in stops]\n",
      "    clean_train_reviews.append(clean_review)\n",
      "    count += trainSentNumList[i]\n",
      "trainDataVecs = getAvgFeatureVecs( clean_train_reviews, model, num_features, trainset )\n",
      "\n",
      "print \"Creating average feature vecs for test reviews\"\n",
      "\n",
      "clean_test_reviews = []\n",
      "for i in xrange(len(testSentNumList)):\n",
      "    chain = itertools.chain(*allList[count:count+testSentNumList[i]])\n",
      "    clean_review = [w for w in list(chain) if not w in stops]\n",
      "    clean_test_reviews.append(clean_review)\n",
      "    count += testSentNumList[i]\n",
      "testDataVecs = getAvgFeatureVecs( clean_test_reviews, model, num_features, testset )\n",
      "'''\n",
      "clean_train_reviews = []\n",
      "for review in train[\"review\"]:\n",
      "    clean_train_reviews.append( review_to_wordlist( review, \\\n",
      "        remove_stopwords=True ))\n",
      "\n",
      "trainDataVecs = getAvgFeatureVecs( clean_train_reviews, model, num_features, trainset )\n",
      "\n",
      "print \"Creating average feature vecs for test reviews\"\n",
      "clean_test_reviews = []\n",
      "for review in test[\"review\"]:\n",
      "    clean_test_reviews.append( review_to_wordlist( review, \\\n",
      "        remove_stopwords=True ))\n",
      "\n",
      "testDataVecs = getAvgFeatureVecs( clean_test_reviews, model, num_features, testset )\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Review 0 of 25000\n",
        "Review 1000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 2000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 3000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 4000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 5000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 6000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 7000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 8000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 9000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 10000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 11000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 12000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 13000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 14000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 15000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 16000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 17000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 18000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 19000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 20000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 21000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 22000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 23000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 24000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Creating average feature vecs for test reviews"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 0 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 1000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 2000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 3000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 4000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 5000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 6000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 7000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 8000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 9000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 10000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 11000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 12000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 13000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 14000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 15000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 16000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 17000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 18000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 19000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 20000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 21000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 22000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 23000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 24000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 290
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "predicting with logistic regression"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y = train[\"sentiment\"]\n",
      "\n",
      "modelLR = LogisticRegression(penalty='l2', dual=True, tol=0.0001,\n",
      "                         C=4, fit_intercept=True, intercept_scaling=1.0,\n",
      "                         class_weight=None, random_state=None)\n",
      "print \"20 Fold CV Score: \", np.mean(cross_validation.cross_val_score(modelLR, trainDataVecs, y, cv=20, scoring='roc_auc'))\n",
      "\n",
      "print \"Retrain on all training data, predicting test labels...\\n\"\n",
      "modelLR.fit(trainDataVecs,y)\n",
      "result = modelLR.predict_proba(testDataVecs)[:,1]\n",
      "output = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":result} )\n",
      "\n",
      "# Use pandas to write the comma-separated output file\n",
      "output.to_csv( \"Word2Vec_AverageVectors.csv\", index=False, quoting=3 )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "20 Fold CV Score:  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.935401728\n",
        "Retrain on all training data, predicting test labels...\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 291
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "0.937176576"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 292,
       "text": [
        "0.937176576"
       ]
      }
     ],
     "prompt_number": 292
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "min_count=1, threshold=1.0, 0.926957696\n",
      "min_count=5, threshold=5.0, 0.932\n",
      "min_count=5, threshold=10.0, 0.93352832\n",
      "min_count=5, threshold=10.0, 0.934248192\n",
      "min_count=5, threshold=50.0, 0.933776512\n",
      "min_count=5, threshold=20.0, 0.93538\n",
      "min_count=10, threshold=20.0, 0.93319232\n",
      "min_count=3, threshold=20.0, 0.93314368"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "SyntaxError",
       "evalue": "can't assign to literal (<ipython-input-293-17c6361ff010>, line 1)",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-293-17c6361ff010>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    min_count=1, threshold=1.0, 0.926957696\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m can't assign to literal\n"
       ]
      }
     ],
     "prompt_number": 293
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print clean_train_reviews[1000]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[u'watched', u'movie', u'really', u'late', u'last', u'night', u'usually', u'late', u'm', u'pretty', u'forgiving', u'movies', u'although', u'tried', u'could', u'stand', u'movie', u'kept', u'getting', u'worse', u'worse', u'movie', u'went', u'although', u'know', u'suppose', u'comedy', u'didn', u'find', u'funny', u'also', u'especially', u'unrealistic', u'jaded', u'portrayal', u'rural', u'life', u'case', u'think', u'country', u'life', u'like', u'definitely', u'agree', u'guy', u'cast', u'members', u'cute', u'french', u'guy', u'really', u'fake', u'agree', u'tried', u'good', u'lesson', u'story', u'overall', u'recommendation', u'one', u'watch', u'annoying']\n"
       ]
      }
     ],
     "prompt_number": 301
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model.most_similar('los angeles',topn=20)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 298,
       "text": [
        "[(u'new york', 0.67737877368927),\n",
        " (u'san francisco', 0.6622974872589111),\n",
        " (u'new jersey', 0.6305004358291626),\n",
        " (u'nyc', 0.6188726425170898),\n",
        " (u'downtown', 0.6042178869247437),\n",
        " (u'connecticut', 0.6008429527282715),\n",
        " (u'city', 0.5978505611419678),\n",
        " (u'chicago', 0.5916966795921326),\n",
        " (u'small town', 0.5903012752532959),\n",
        " (u'vermont', 0.5811640620231628),\n",
        " (u'new mexico', 0.5741614103317261),\n",
        " (u'hometown', 0.572705864906311),\n",
        " (u'southern california', 0.5698432922363281),\n",
        " (u'area', 0.5696370005607605),\n",
        " (u'california', 0.5695796012878418),\n",
        " (u'pennsylvania', 0.5672639608383179),\n",
        " (u'boston', 0.5667223930358887),\n",
        " (u'atlanta', 0.5627097487449646),\n",
        " (u'queens', 0.5597313642501831),\n",
        " (u'times square', 0.5593491196632385)]"
       ]
      }
     ],
     "prompt_number": 298
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model.most_similar('cinematography',topn=20)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 18,
       "text": [
        "[(u'photography', 0.8151588439941406),\n",
        " (u'art direction', 0.798570990562439),\n",
        " (u'camera work', 0.7376245856285095),\n",
        " (u'set design', 0.7294052839279175),\n",
        " (u'music score', 0.7032378911972046),\n",
        " (u'costumes sets', 0.6979056000709534),\n",
        " (u'camerawork', 0.6957721710205078),\n",
        " (u'costume design', 0.6933091282844543),\n",
        " (u'production design', 0.685977041721344),\n",
        " (u'use shadows', 0.6808332800865173),\n",
        " (u'set decoration', 0.6720283627510071),\n",
        " (u'lighting', 0.6658917665481567),\n",
        " (u'jerry goldsmith', 0.6643579602241516),\n",
        " (u'camera movements', 0.6640276312828064),\n",
        " (u'tracking shots', 0.6609379053115845),\n",
        " (u'angles keeping', 0.6570247411727905),\n",
        " (u'stunning cinematography', 0.656588613986969),\n",
        " (u'crisp', 0.6564950942993164),\n",
        " (u'enhances', 0.655102014541626),\n",
        " (u'direction', 0.6537296772003174)]"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}