{
 "metadata": {
  "name": "",
  "signature": "sha256:c31ae433895ba7f6a4ca47f602ca90842d7db8236c1b662e894144d2adeae979"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import numpy as np\n",
      "train = pd.read_csv('labeledTrainData.tsv', header=0, delimiter=\"\\t\", quoting=3)\n",
      "unlabeled_train = pd.read_csv('unlabeledTrainData.tsv', header=0, delimiter=\"\\t\", quoting=3)\n",
      "test = pd.read_csv('testData.tsv', header=0, delimiter=\"\\t\", quoting=3 )\n",
      "y = train[\"sentiment\"]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "calcualte tfidf score"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "from KaggleWord2VecUtility import KaggleWord2VecUtility\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn import cross_validation\n",
      "print \"Cleaning and parsing movie reviews...\\n\"\n",
      "traindata = []\n",
      "for i in xrange( 0, len(train[\"review\"])):\n",
      "    traindata.append(\" \".join(KaggleWord2VecUtility.review_to_wordlist(train[\"review\"][i], True)))\n",
      "testdata = []\n",
      "for i in xrange(0,len(test[\"review\"])):\n",
      "    testdata.append(\" \".join(KaggleWord2VecUtility.review_to_wordlist(test[\"review\"][i], True)))\n",
      "unlabeled_traindata = []\n",
      "for i in xrange(0,len(unlabeled_train[\"review\"])):\n",
      "    unlabeled_traindata.append(\" \".join(KaggleWord2VecUtility.review_to_wordlist(unlabeled_train[\"review\"][i], True)))\n",
      "print 'vectorizing... ' \n",
      "tfv = TfidfVectorizer(min_df=1,  max_features=None, \n",
      "        strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
      "        ngram_range=(1, 1), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
      "        stop_words = 'english')\n",
      "alldata = traindata + testdata + unlabeled_traindata\n",
      "lentrain = len(traindata)\n",
      "lentest = len(testdata)\n",
      "print \"fitting pipeline... \"\n",
      "tfv.fit(alldata)\n",
      "alldata = tfv.transform(alldata)\n",
      "trainset = alldata[:lentrain]\n",
      "testset = alldata[lentrain:(lentrain+lentest)]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Cleaning and parsing movie reviews...\n",
        "\n",
        "vectorizing... "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "fitting pipeline... \n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(tfv.get_feature_names())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 46,
       "text": [
        "141055"
       ]
      }
     ],
     "prompt_number": 46
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "building word2vec model"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "'''\n",
      "# Read data from files \n",
      "train = pd.read_csv( \"labeledTrainData.tsv\", header=0, \n",
      " delimiter=\"\\t\", quoting=3 )\n",
      "test = pd.read_csv( \"testData.tsv\", header=0, delimiter=\"\\t\", quoting=3 )\n",
      "unlabeled_train = pd.read_csv( \"unlabeledTrainData.tsv\", header=0, \n",
      " delimiter=\"\\t\", quoting=3 )\n",
      "\n",
      "# Verify the number of reviews that were read (100,000 in total)\n",
      "print \"Read %d labeled train reviews, %d labeled test reviews, \" \\\n",
      " \"and %d unlabeled reviews\\n\" % (train[\"review\"].size,  \n",
      " test[\"review\"].size, unlabeled_train[\"review\"].size )\n",
      "print train.shape, test.shape, unlabeled_train.shape\n",
      "'''\n",
      "# Import various modules for string cleaning\n",
      "from bs4 import BeautifulSoup\n",
      "import re\n",
      "from nltk.corpus import stopwords\n",
      "\n",
      "def review_to_wordlist( review, remove_stopwords=False ):\n",
      "    # Function to convert a document to a sequence of words,\n",
      "    # optionally removing stop words.  Returns a list of words.\n",
      "    #\n",
      "    # 1. Remove HTML\n",
      "    review_text = BeautifulSoup(review).get_text()\n",
      "    #  \n",
      "    # 2. Remove non-letters\n",
      "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
      "    #\n",
      "    # 3. Convert words to lower case and split them\n",
      "    words = review_text.lower().split()\n",
      "    #\n",
      "    # 4. Optionally remove stop words (false by default)\n",
      "    if remove_stopwords:\n",
      "        stops = set(stopwords.words(\"english\"))\n",
      "        words = [w for w in words if not w in stops]\n",
      "    #\n",
      "    # 5. Return a list of words\n",
      "    return(words)\n",
      "\n",
      "# Download the punkt tokenizer for sentence splitting\n",
      "import nltk.data\n",
      "#nltk.download()   \n",
      "\n",
      "# Load the punkt tokenizer\n",
      "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
      "\n",
      "# Define a function to split a review into parsed sentences\n",
      "def review_to_sentences( review, tokenizer, remove_stopwords=False ):\n",
      "    # Function to split a review into parsed sentences. Returns a \n",
      "    # list of sentences, where each sentence is a list of words\n",
      "    #\n",
      "    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
      "    raw_sentences = tokenizer.tokenize(review.decode('utf-8').strip())\n",
      "    #\n",
      "    # 2. Loop over each sentence\n",
      "    sentences = []\n",
      "    for raw_sentence in raw_sentences:\n",
      "        # If a sentence is empty, skip it\n",
      "        if len(raw_sentence) > 0:\n",
      "            # Otherwise, call review_to_wordlist to get a list of words\n",
      "            sentences.append( review_to_wordlist( raw_sentence, remove_stopwords ))\n",
      "    #\n",
      "    # Return the list of sentences (each sentence is a list of words,\n",
      "    # so this returns a list of lists\n",
      "    return sentences\n",
      "\n",
      "\n",
      "reviewList = []  # Initialize an empty list of sentences\n",
      "\n",
      "print \"Parsing sentences from training set\"\n",
      "for review in train[\"review\"]:\n",
      "    reviewList += review_to_sentences(review, tokenizer, False)\n",
      "\n",
      "print \"Parsing sentences from unlabeled set\"\n",
      "for review in unlabeled_train[\"review\"]:\n",
      "    reviewList += review_to_sentences(review, tokenizer, False)\n",
      "\n",
      "\n",
      "# Import the built-in logging module and configure it so that Word2Vec \n",
      "# creates nice output messages\n",
      "import logging\n",
      "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
      "    level=logging.INFO)\n",
      "\n",
      "# Set values for various parameters\n",
      "num_features = 200    # Word vector dimensionality                      \n",
      "min_word_count = 10   # Minimum word count                        \n",
      "num_workers = 4       # Number of threads to run in parallel\n",
      "context = 14          # Context window size                                                                                    \n",
      "downsampling = 1e-4   # Downsample setting for frequent words\n",
      "\n",
      "# Initialize and train the model (this will take some time)\n",
      "from gensim.models import word2vec\n",
      "print \"Training model...\"\n",
      "model = word2vec.Word2Vec(reviewList, workers=num_workers, \\\n",
      "            size=num_features, min_count = min_word_count, \\\n",
      "            window = context, sample = downsampling)\n",
      "\n",
      "# If you don't plan to train the model any further, calling \n",
      "# init_sims will make the model much more memory-efficient.\n",
      "model.init_sims(replace=True)\n",
      "\n",
      "# It can be helpful to create a meaningful model name and \n",
      "# save the model for later use. You can load it later using Word2Vec.load()\n",
      "model_name = \"300features_40minwords_10context\"\n",
      "model.save(model_name)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Parsing sentences from training set\n",
        "Parsing sentences from unlabeled set"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Training model..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 39
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "vectorizing all words"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "# model = word2vec.Word2Vec.load(\"300features_40minwords_10context\")\n",
      "import numpy as np  # Make sure that numpy is imported\n",
      "tfv_feature_name_list = tfv.get_feature_names()\n",
      "def makeFeatureVec(words, model, num_features, tfv_feature_name_list, tfidfVec):\n",
      "    # Function to average all of the word vectors in a given\n",
      "    # paragraph\n",
      "    #\n",
      "    # Pre-initialize an empty numpy array (for speed)\n",
      "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
      "    #\n",
      "    # \n",
      "    # Index2word is a list that contains the names of the words in \n",
      "    # the model's vocabulary. Convert it to a set, for speed \n",
      "    index2word_set = set(model.index2word)\n",
      "    #\n",
      "    tfv_feature_name = [tfv_feature_name_list[i] for i in tfidfVec.indices];\n",
      "    #\n",
      "    # Loop over each word in the review and, if it is in the model's\n",
      "    # vocaublary, add its feature vector to the total\n",
      "    weight = 0\n",
      "    for i in xrange(len(tfv_feature_name)):\n",
      "        if tfv_feature_name[i] in index2word_set: \n",
      "            weight += tfidfVec.data[i]\n",
      "            featureVec = np.add(featureVec,model[tfv_feature_name[i]]*tfidfVec.data[i])\n",
      "    # \n",
      "    # Divide the result by the number of words to get the average\n",
      "    featureVec = np.divide(featureVec,weight)\n",
      "    return featureVec\n",
      "\n",
      "\n",
      "def getAvgFeatureVecs(reviews, model, num_features, tfidfMat):\n",
      "    # Given a set of reviews (each one a list of words), calculate \n",
      "    # the average feature vector for each one and return a 2D numpy array \n",
      "    # \n",
      "    # Initialize a counter\n",
      "    counter = 0\n",
      "    # \n",
      "    # Preallocate a 2D numpy array, for speed\n",
      "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
      "    # \n",
      "    # Loop through the reviews\n",
      "    for review in reviews:\n",
      "       #\n",
      "       # Print a status message every 1000th review\n",
      "       if counter%1000 == 0:\n",
      "           print \"Review %d of %d\" % (counter, len(reviews))\n",
      "       # \n",
      "       # Call the function (defined above) that makes average feature vectors\n",
      "       reviewFeatureVecs[counter] = makeFeatureVec(review, model, \\\n",
      "           num_features, tfv_feature_name_list, tfidfMat.getrow(counter))\n",
      "       #\n",
      "       # Increment the counter\n",
      "       counter = counter + 1\n",
      "    return reviewFeatureVecs\n",
      "\n",
      "# ****************************************************************\n",
      "# Calculate average feature vectors for training and testing sets,\n",
      "# using the functions we defined above. Notice that we now use stop word\n",
      "# removal.\n",
      "\n",
      "clean_train_reviews = []\n",
      "for review in train[\"review\"]:\n",
      "    clean_train_reviews.append( review_to_wordlist( review, \\\n",
      "        remove_stopwords=True ))\n",
      "\n",
      "trainDataVecs = getAvgFeatureVecs( clean_train_reviews, model, num_features, trainset )\n",
      "\n",
      "print \"Creating average feature vecs for test reviews\"\n",
      "clean_test_reviews = []\n",
      "for review in test[\"review\"]:\n",
      "    clean_test_reviews.append( review_to_wordlist( review, \\\n",
      "        remove_stopwords=True ))\n",
      "\n",
      "testDataVecs = getAvgFeatureVecs( clean_test_reviews, model, num_features, testset )\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Review 0 of 25000\n",
        "Review 1000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 2000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 3000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 4000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 5000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 6000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 7000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 8000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 9000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 10000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 11000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 12000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 13000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 14000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 15000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 16000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 17000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 18000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 19000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 20000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 21000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 22000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 23000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 24000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Creating average feature vecs for test reviews"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 0 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 1000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 2000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 3000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 4000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 5000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 6000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 7000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 8000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 9000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 10000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 11000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 12000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 13000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 14000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 15000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 16000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 17000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 18000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 19000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 20000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 21000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 22000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 23000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 24000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 40
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "predicting with logistic regression"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y = train[\"sentiment\"]\n",
      "\n",
      "modelLR = LogisticRegression(penalty='l2', dual=True, tol=0.0001,\n",
      "                         C=4, fit_intercept=True, intercept_scaling=1.0,\n",
      "                         class_weight=None, random_state=None)\n",
      "print \"20 Fold CV Score: \", np.mean(cross_validation.cross_val_score(modelLR, trainDataVecs, y, cv=20, scoring='roc_auc'))\n",
      "\n",
      "print \"Retrain on all training data, predicting test labels...\\n\"\n",
      "modelLR.fit(trainDataVecs,y)\n",
      "result = modelLR.predict_proba(testDataVecs)[:,1]\n",
      "output = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":result} )\n",
      "\n",
      "# Use pandas to write the comma-separated output file\n",
      "output.to_csv( \"Word2Vec_AverageVectors.csv\", index=False, quoting=3 )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "20 Fold CV Score:  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.937336448\n",
        "Retrain on all training data, predicting test labels...\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 53
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model.most_similar('classic')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 45,
       "text": [
        "[(u'classics', 0.6514070630073547),\n",
        " (u'cult', 0.5934808254241943),\n",
        " (u'masterpiece', 0.5375394821166992),\n",
        " (u'genre', 0.5222079753875732),\n",
        " (u'timeless', 0.4829285740852356),\n",
        " (u'remake', 0.4810746908187866),\n",
        " (u'original', 0.46791285276412964),\n",
        " (u's', 0.45641523599624634),\n",
        " (u'films', 0.45602768659591675),\n",
        " (u'wonderful', 0.4550446569919586)]"
       ]
      }
     ],
     "prompt_number": 45
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model['classic']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 43,
       "text": [
        "array([-0.11062193, -0.07743458,  0.00126183, -0.04623244,  0.02848539,\n",
        "       -0.06304399, -0.07540859,  0.14571038,  0.03733743, -0.10804461,\n",
        "       -0.04110803, -0.02646492,  0.06866748,  0.07778584,  0.02599656,\n",
        "       -0.0384142 ,  0.03160953, -0.06904119,  0.05259167,  0.04398924,\n",
        "        0.00223686, -0.05465718,  0.0160176 , -0.06812865,  0.14858577,\n",
        "       -0.00126202, -0.00695668, -0.09248453, -0.01494078,  0.0038696 ,\n",
        "        0.01156113, -0.02053781, -0.0119993 ,  0.00748331,  0.0790223 ,\n",
        "       -0.03203076,  0.05332886,  0.14916511, -0.13597369,  0.07883128,\n",
        "       -0.01083672, -0.18343392,  0.05638254,  0.0005102 ,  0.01354296,\n",
        "        0.07412686, -0.17575482,  0.08001062, -0.02877334, -0.06662642,\n",
        "       -0.06054551,  0.08427935,  0.03767878, -0.00418885,  0.00613455,\n",
        "        0.10638829, -0.10494864,  0.03940395, -0.05371693,  0.05581176,\n",
        "       -0.01319427, -0.0594039 , -0.01720581, -0.05266125,  0.00290957,\n",
        "        0.00888494,  0.06312899, -0.01509484, -0.03392702,  0.0101156 ,\n",
        "        0.04816182, -0.04371947,  0.0571786 , -0.19969136, -0.06898959,\n",
        "       -0.13406718,  0.01893237,  0.02568326,  0.17145377,  0.09861422,\n",
        "        0.10508686, -0.01461562, -0.08339024, -0.02569842,  0.04241795,\n",
        "       -0.12042623,  0.14292479,  0.00301872,  0.05868254, -0.01147427,\n",
        "        0.13840561, -0.03368942,  0.05905098,  0.03858776, -0.06117442,\n",
        "        0.05114007,  0.06390768, -0.05209212, -0.05525684,  0.03187038,\n",
        "       -0.13412401,  0.02888586,  0.0398759 ,  0.09100209,  0.08752128,\n",
        "        0.09124707,  0.0206174 ,  0.03438311,  0.04791833, -0.00872803,\n",
        "       -0.02925163,  0.07496323,  0.00292121, -0.02208945, -0.00345932,\n",
        "        0.03649259,  0.08166727,  0.03919257, -0.07766151, -0.08968303,\n",
        "        0.01369007, -0.01817277, -0.03685129, -0.03422128,  0.01899887,\n",
        "       -0.011256  , -0.02767481, -0.03906685,  0.06555495,  0.07271873,\n",
        "       -0.04733157, -0.1777063 , -0.03061999,  0.01875132,  0.02501315,\n",
        "        0.03022142, -0.01774892, -0.044848  ,  0.01054402, -0.02691063,\n",
        "       -0.10965396, -0.06951281,  0.12688486, -0.02360002, -0.12317174,\n",
        "       -0.01766251, -0.00411368, -0.09601413, -0.03999836,  0.04436499,\n",
        "       -0.0395606 ,  0.03282746, -0.06471051,  0.0046907 ,  0.1161544 ,\n",
        "       -0.04865709, -0.02592696,  0.01817075, -0.0044033 , -0.00597299,\n",
        "        0.02604873,  0.07943097, -0.00650804,  0.15957396, -0.0088774 ,\n",
        "        0.05307978,  0.10080471, -0.02806482,  0.01294618, -0.15156576,\n",
        "       -0.02470488, -0.08965444, -0.07397159,  0.09476529, -0.12670869,\n",
        "       -0.04495891, -0.11693817,  0.0374855 , -0.03317858, -0.03792869,\n",
        "        0.03309379,  0.05880603,  0.11926199, -0.00403598,  0.05354244,\n",
        "       -0.01960315,  0.02711586,  0.0407965 , -0.01737272, -0.06724165,\n",
        "       -0.17120935, -0.01915028, -0.1807766 , -0.10498472,  0.01175785,\n",
        "        0.01929192, -0.07325318,  0.0317202 , -0.0476445 ,  0.02761865], dtype=float32)"
       ]
      }
     ],
     "prompt_number": 43
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(reviewList[124050])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "18\n"
       ]
      }
     ],
     "prompt_number": 51
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from gensim.models import word2vec\n",
      "model = word2vec.Word2Vec.load(\"300features_40minwords_10context\")\n",
      "plot_list = [item[0] for item in model.most_similar('story',topn=20)]\n",
      "plot_list"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 13,
       "text": [
        "[u'plot',\n",
        " u'storyline',\n",
        " u'stories',\n",
        " u'simple',\n",
        " u'tale',\n",
        " u'very',\n",
        " u'told',\n",
        " u'events',\n",
        " u'premise',\n",
        " u'telling',\n",
        " u'interesting',\n",
        " u'structure',\n",
        " u'script',\n",
        " u'details',\n",
        " u'movie',\n",
        " u'which',\n",
        " u'compelling',\n",
        " u'the',\n",
        " u'characters',\n",
        " u'itself']"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cine_list = [item[0] for item in model.most_similar('cinematography',topn=20)]\n",
      "cine_list"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "[u'photography',\n",
        " u'direction',\n",
        " u'visuals',\n",
        " u'lighting',\n",
        " u'camerawork',\n",
        " u'composition',\n",
        " u'editing',\n",
        " u'scenery',\n",
        " u'costuming',\n",
        " u'score',\n",
        " u'colors',\n",
        " u'lush',\n",
        " u'breathtaking',\n",
        " u'decoration',\n",
        " u'design',\n",
        " u'visually',\n",
        " u'settings',\n",
        " u'locations',\n",
        " u'visual',\n",
        " u'evocative']"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "char_list = [item[0] for item in model.most_similar('characters',topn=20)]\n",
      "char_list"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 12,
       "text": [
        "[u'character',\n",
        " u'personalities',\n",
        " u'dimensional',\n",
        " u'storyline',\n",
        " u'development',\n",
        " u'developed',\n",
        " u'develop',\n",
        " u'subplots',\n",
        " u'situations',\n",
        " u'main',\n",
        " u'fleshed',\n",
        " u'believable',\n",
        " u'interaction',\n",
        " u'underdeveloped',\n",
        " u'none',\n",
        " u'likable',\n",
        " u'protagonists',\n",
        " u'actors',\n",
        " u'depth',\n",
        " u'unlikeable']"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "total_list = plot_list + cine_list + char_list\n",
      "total_list"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 15,
       "text": [
        "[u'plot',\n",
        " u'storyline',\n",
        " u'stories',\n",
        " u'simple',\n",
        " u'tale',\n",
        " u'very',\n",
        " u'told',\n",
        " u'events',\n",
        " u'premise',\n",
        " u'telling',\n",
        " u'interesting',\n",
        " u'structure',\n",
        " u'script',\n",
        " u'details',\n",
        " u'movie',\n",
        " u'which',\n",
        " u'compelling',\n",
        " u'the',\n",
        " u'characters',\n",
        " u'itself',\n",
        " u'photography',\n",
        " u'direction',\n",
        " u'visuals',\n",
        " u'lighting',\n",
        " u'camerawork',\n",
        " u'composition',\n",
        " u'editing',\n",
        " u'scenery',\n",
        " u'costuming',\n",
        " u'score',\n",
        " u'colors',\n",
        " u'lush',\n",
        " u'breathtaking',\n",
        " u'decoration',\n",
        " u'design',\n",
        " u'visually',\n",
        " u'settings',\n",
        " u'locations',\n",
        " u'visual',\n",
        " u'evocative',\n",
        " u'character',\n",
        " u'personalities',\n",
        " u'dimensional',\n",
        " u'storyline',\n",
        " u'development',\n",
        " u'developed',\n",
        " u'develop',\n",
        " u'subplots',\n",
        " u'situations',\n",
        " u'main',\n",
        " u'fleshed',\n",
        " u'believable',\n",
        " u'interaction',\n",
        " u'underdeveloped',\n",
        " u'none',\n",
        " u'likable',\n",
        " u'protagonists',\n",
        " u'actors',\n",
        " u'depth',\n",
        " u'unlikeable']"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}