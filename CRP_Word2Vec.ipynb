{
 "metadata": {
  "name": "",
  "signature": "sha256:542d5adbeb81a730717224932e2294e170edda17eca9629f42a1ca7688de0b2f"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "import data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "\n",
      "# Read data from files \n",
      "train = pd.read_csv( \"labeledTrainData.tsv\", header=0, \n",
      " delimiter=\"\\t\", quoting=3 )\n",
      "test = pd.read_csv( \"testData.tsv\", header=0, delimiter=\"\\t\", quoting=3 )\n",
      "unlabeled_train = pd.read_csv( \"unlabeledTrainData.tsv\", header=0, \n",
      " delimiter=\"\\t\", quoting=3 )\n",
      "\n",
      "# Verify the number of reviews that were read (100,000 in total)\n",
      "print \"Read %d labeled train reviews, %d labeled test reviews, \" \\\n",
      " \"and %d unlabeled reviews\\n\" % (train[\"review\"].size,  \n",
      " test[\"review\"].size, unlabeled_train[\"review\"].size )\n",
      "print train.shape, test.shape, unlabeled_train.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Read 25000 labeled train reviews, 25000 labeled test reviews, and 50000 unlabeled reviews\n",
        "\n",
        "(25000, 3) (25000, 2) (50000, 2)\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "define functions to generate words and sentences lists"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Import various modules for string cleaning\n",
      "from bs4 import BeautifulSoup\n",
      "import re\n",
      "from nltk.corpus import stopwords\n",
      "\n",
      "def review_to_wordlist( review, remove_stopwords=False ):\n",
      "    # Function to convert a document to a sequence of words,\n",
      "    # optionally removing stop words.  Returns a list of words.\n",
      "    #\n",
      "    # 1. Remove HTML\n",
      "    review_text = BeautifulSoup(review).get_text()\n",
      "    #  \n",
      "    # 2. Remove non-letters\n",
      "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
      "    #\n",
      "    # 3. Convert words to lower case and split them\n",
      "    words = review_text.lower().split()\n",
      "    #\n",
      "    # 4. Optionally remove stop words (false by default)\n",
      "    if remove_stopwords:\n",
      "        stops = set(stopwords.words(\"english\"))\n",
      "        words = [w for w in words if not w in stops]\n",
      "    #\n",
      "    # 5. Return a list of words\n",
      "    return(words)\n",
      "\n",
      "# Download the punkt tokenizer for sentence splitting\n",
      "import nltk.data\n",
      "#nltk.download()   \n",
      "\n",
      "# Load the punkt tokenizer\n",
      "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
      "\n",
      "# Define a function to split a review into parsed sentences\n",
      "def review_to_sentences( review, tokenizer, remove_stopwords=False ):\n",
      "    # Function to split a review into parsed sentences. Returns a \n",
      "    # list of sentences, where each sentence is a list of words\n",
      "    #\n",
      "    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
      "    raw_sentences = tokenizer.tokenize(review.decode('utf-8').strip())\n",
      "    #\n",
      "    # 2. Loop over each sentence\n",
      "    sentences = []\n",
      "    for raw_sentence in raw_sentences:\n",
      "        # If a sentence is empty, skip it\n",
      "        if len(raw_sentence) > 0:\n",
      "            # Otherwise, call review_to_wordlist to get a list of words\n",
      "            sentences.append( review_to_wordlist( raw_sentence, remove_stopwords ))\n",
      "    #\n",
      "    # Return the list of sentences (each sentence is a list of words,\n",
      "    # so this returns a list of lists\n",
      "    return sentences"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "preparing data for word2vec model"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "reviewList = []  # Initialize an empty list of sentences\n",
      "\n",
      "print \"Parsing sentences from training set\"\n",
      "for review in train[\"review\"]:\n",
      "    reviewList += review_to_sentences(review, tokenizer)\n",
      "\n",
      "print \"Parsing sentences from unlabeled set\"\n",
      "for review in unlabeled_train[\"review\"]:\n",
      "    reviewList += review_to_sentences(review, tokenizer)\n",
      "\n",
      "print \"Parsing sentences from test set\"\n",
      "for review in test[\"review\"]:\n",
      "    reviewList += review_to_sentences(review, tokenizer)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/Users/yunfengxi/anaconda/lib/python2.7/site-packages/bs4/__init__.py:169: UserWarning: \".\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
        "  '\"%s\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.' % markup)\n",
        "/Users/yunfengxi/anaconda/lib/python2.7/site-packages/bs4/__init__.py:176: UserWarning: \"http://www.happierabroad.com\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
        "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Parsing sentences from training set\n",
        "Parsing sentences from unlabeled set"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/Users/yunfengxi/anaconda/lib/python2.7/site-packages/bs4/__init__.py:176: UserWarning: \"http://www.archive.org/details/LovefromaStranger\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
        "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
        "/Users/yunfengxi/anaconda/lib/python2.7/site-packages/bs4/__init__.py:176: UserWarning: \"http://www.loosechangeguide.com/LooseChangeGuide.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
        "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/Users/yunfengxi/anaconda/lib/python2.7/site-packages/bs4/__init__.py:176: UserWarning: \"http://www.msnbc.msn.com/id/4972055/site/newsweek/\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
        "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
        "/Users/yunfengxi/anaconda/lib/python2.7/site-packages/bs4/__init__.py:169: UserWarning: \"..\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
        "  '\"%s\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.' % markup)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/Users/yunfengxi/anaconda/lib/python2.7/site-packages/bs4/__init__.py:176: UserWarning: \"http://www.youtube.com/watch?v=a0KSqelmgN8\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
        "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
        "/Users/yunfengxi/anaconda/lib/python2.7/site-packages/bs4/__init__.py:176: UserWarning: \"http://jake-weird.blogspot.com/2007/08/beneath.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
        "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Parsing sentences from test set"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "build word2vec model"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Import the built-in logging module and configure it so that Word2Vec \n",
      "# creates nice output messages\n",
      "import logging\n",
      "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
      "    level=logging.INFO)\n",
      "import scipy\n",
      "# Set values for various parameters\n",
      "num_features = 300    # Word vector dimensionality                      \n",
      "min_word_count = 1   # Minimum word count                        \n",
      "num_workers = 4       # Number of threads to run in parallel\n",
      "context = 10          # Context window size                                                                                    \n",
      "downsampling = 1e-2   # Downsample setting for frequent words\n",
      "\n",
      "# Initialize and train the model (this will take some time)\n",
      "from gensim.models import word2vec\n",
      "print \"Training model...\"\n",
      "model = word2vec.Word2Vec(reviewList, workers=num_workers, \\\n",
      "            size=num_features, min_count = min_word_count, \\\n",
      "            window = context, sample = downsampling)\n",
      "\n",
      "# If you don't plan to train the model any further, calling \n",
      "# init_sims will make the model much more memory-efficient.\n",
      "model.init_sims(replace=True)\n",
      "\n",
      "# It can be helpful to create a meaningful model name and \n",
      "# save the model for later use. You can load it later using Word2Vec.load()\n",
      "model_name = \"300features_40minwords_10context\"\n",
      "model.save(model_name)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Training model...\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/Users/yunfengxi/anaconda/lib/python2.7/site-packages/numpy/lib/utils.py:95: DeprecationWarning: `scipy.sparse.sparsetools` is deprecated!\n",
        "scipy.sparse.sparsetools is a private module for scipy.sparse, and should not be used.\n",
        "  warnings.warn(depdoc, DeprecationWarning)\n",
        "/Users/yunfengxi/anaconda/lib/python2.7/site-packages/scipy/lib/_util.py:67: DeprecationWarning: Module scipy.linalg.blas.fblas is deprecated, use scipy.linalg.blas instead\n",
        "  DeprecationWarning)\n",
        "/Users/yunfengxi/anaconda/lib/python2.7/site-packages/scipy/lib/_util.py:67: DeprecationWarning: Module scipy.linalg.blas.fblas is deprecated, use scipy.linalg.blas instead\n",
        "  DeprecationWarning)\n",
        "/Users/yunfengxi/anaconda/lib/python2.7/site-packages/scipy/lib/_util.py:67: DeprecationWarning: Module scipy.linalg.blas.fblas is deprecated, use scipy.linalg.blas instead\n",
        "  DeprecationWarning)\n",
        "/Users/yunfengxi/anaconda/lib/python2.7/site-packages/scipy/lib/_util.py:67: DeprecationWarning: Module scipy.linalg.blas.fblas is deprecated, use scipy.linalg.blas instead\n",
        "  DeprecationWarning)\n",
        "/Users/yunfengxi/anaconda/lib/python2.7/site-packages/scipy/lib/_util.py:67: DeprecationWarning: Module scipy.linalg.blas.fblas is deprecated, use scipy.linalg.blas instead\n",
        "  DeprecationWarning)\n",
        "/Users/yunfengxi/anaconda/lib/python2.7/site-packages/scipy/lib/_util.py:67: DeprecationWarning: Module scipy.linalg.blas.fblas is deprecated, use scipy.linalg.blas instead\n",
        "  DeprecationWarning)\n",
        "/Users/yunfengxi/anaconda/lib/python2.7/site-packages/scipy/lib/_util.py:67: DeprecationWarning: Module scipy.linalg.blas.fblas is deprecated, use scipy.linalg.blas instead\n",
        "  DeprecationWarning)\n",
        "/Users/yunfengxi/anaconda/lib/python2.7/site-packages/scipy/lib/_util.py:67: DeprecationWarning: Module scipy.linalg.blas.fblas is deprecated, use scipy.linalg.blas instead\n",
        "  DeprecationWarning)\n",
        "/Users/yunfengxi/anaconda/lib/python2.7/site-packages/scipy/lib/_util.py:67: DeprecationWarning: Module scipy.linalg.blas.fblas is deprecated, use scipy.linalg.blas instead\n",
        "  DeprecationWarning)\n",
        "/Users/yunfengxi/anaconda/lib/python2.7/site-packages/scipy/lib/_util.py:67: DeprecationWarning: Module scipy.linalg.blas.fblas is deprecated, use scipy.linalg.blas instead\n",
        "  DeprecationWarning)\n",
        "/Users/yunfengxi/anaconda/lib/python2.7/site-packages/scipy/lib/_util.py:67: DeprecationWarning: Module scipy.linalg.blas.fblas is deprecated, use scipy.linalg.blas instead\n",
        "  DeprecationWarning)\n",
        "/Users/yunfengxi/anaconda/lib/python2.7/site-packages/scipy/lib/_util.py:67: DeprecationWarning: Module scipy.linalg.blas.fblas is deprecated, use scipy.linalg.blas instead\n",
        "  DeprecationWarning)\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "CRP functions and vector averaging functions"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# clusterizing each review with Chinese Restaurant Process (CRP)\n",
      "import numpy as np\n",
      "def crp(review, model):\n",
      "    clusterVec = []         # tracks sum of vectors in a cluster\n",
      "    clusterIdx = []         # array of index arrays. e.g. [[1, 3, 5], [2, 4, 6]]\n",
      "    ncluster = 0\n",
      "    # probablity to create a new table if new customer\n",
      "    # is not strongly \"similar\" to any existing table\n",
      "    # word_list = [w for w in review if w in model.index2word]\n",
      "    word_list = review\n",
      "    pnew = 1.0/ (1 + ncluster)  \n",
      "    N = len(word_list)\n",
      "    rands = np.random.rand(N)         # N rand variables sampled from U(0, 1)\n",
      "\n",
      "    for i in range(N):\n",
      "        maxSim = float(\"-infinity\")\n",
      "        maxIdx = 0\n",
      "        v = model[word_list[i]]\n",
      "        if ncluster == 0:\n",
      "            clusterVec.append(v)\n",
      "            clusterIdx.append([i])\n",
      "            ncluster += 1\n",
      "            pnew = 1.0 / (1 + ncluster) \n",
      "            continue\n",
      "        for j in range(ncluster):\n",
      "            sim = np.inner(v, clusterVec[j])/(np.linalg.norm(v)*np.linalg.norm(clusterVec[j]))\n",
      "            if sim > maxSim:\n",
      "                maxIdx = j\n",
      "                maxSim = sim\n",
      "        if maxSim > pnew or (maxSim <= pnew and rands[i] >= pnew):\n",
      "            clusterVec[maxIdx] = clusterVec[maxIdx] + v\n",
      "            clusterIdx[maxIdx].append(i)\n",
      "        else:\n",
      "            clusterVec.append(v)\n",
      "            clusterIdx.append([i])\n",
      "            ncluster += 1\n",
      "            pnew = 1.0 / (1 + ncluster)\n",
      "    return clusterIdx\n",
      "\n",
      "# check words in each review see if they are in trained model, return words only in model\n",
      "def checkWordInModel(review, model):\n",
      "    index2word_set = set(model.index2word)\n",
      "    newReview = []\n",
      "    for word in review:\n",
      "        if word in index2word_set:\n",
      "            newReview.append(word)\n",
      "    return newReview\n",
      "\n",
      "# only keep the closest cluster \n",
      "def findClosestCluster(review, model):\n",
      "    maxSim = float('-infinity')\n",
      "    maxIdx = -1\n",
      "    #newReview = checkWordInModel(review, model)\n",
      "    newReview = review\n",
      "    clusterIdx = crp(newReview, model)\n",
      "    for i in range(len(clusterIdx)):\n",
      "        word_cluster = [newReview[w] for w in clusterIdx[i]]\n",
      "        thisSim = model.n_similarity(word_cluster, ['movie','film','review'])\n",
      "        if thisSim > maxSim:\n",
      "            maxSim = thisSim\n",
      "            maxIdx = i\n",
      "    return [newReview[j] for j in clusterIdx[maxIdx]]\n",
      "\n",
      "# leave out the furthest cluster\n",
      "def removeFurthestCluster(review, model):\n",
      "    minSim = float('infinity')\n",
      "    minIdx = -1\n",
      "    #newReview = checkWordInModel(review, model)\n",
      "    newReview = review\n",
      "    clusterIdx = crp(newReview, model)\n",
      "    if len(clusterIdx) <= 1:\n",
      "        import itertools\n",
      "        chain = itertools.chain(*clusterIdx)\n",
      "        clusterIdx = list(chain)\n",
      "        return [newReview[j] for j in clusterIdx]\n",
      "    for i in range(len(clusterIdx)):\n",
      "        word_cluster = [newReview[w] for w in clusterIdx[i]]\n",
      "        thisSim = model.n_similarity(word_cluster, ['movie','film','review'])\n",
      "        if thisSim < minSim:\n",
      "            minSim = thisSim\n",
      "            minIdx = i\n",
      "    del clusterIdx[minIdx]\n",
      "    import itertools\n",
      "    chain = itertools.chain(*clusterIdx)\n",
      "    clusterIdx = list(chain)\n",
      "    return [newReview[j] for j in clusterIdx]\n",
      "\n",
      "\n",
      "def makeFeatureVec(words, model, num_features):\n",
      "    # Function to average all of the word vectors in a given\n",
      "    # paragraph\n",
      "    #\n",
      "    # Pre-initialize an empty numpy array (for speed)\n",
      "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
      "    #\n",
      "    nwords = 0\n",
      "    # \n",
      "    # Index2word is a list that contains the names of the words in \n",
      "    # the model's vocabulary. Convert it to a set, for speed \n",
      "    index2word_set = set(model.index2word)\n",
      "    #\n",
      "    # Loop over each word in the review and, if it is in the model's\n",
      "    # vocaublary, add its feature vector to the total\n",
      "    for word in words:\n",
      "        if word in index2word_set: \n",
      "            nwords = nwords + 1\n",
      "            featureVec = np.add(featureVec,model[word])\n",
      "    # \n",
      "    # Divide the result by the number of words to get the average\n",
      "    featureVec = np.divide(featureVec,nwords)\n",
      "    return featureVec\n",
      "\n",
      "\n",
      "def getAvgFeatureVecs(reviews, model, num_features):\n",
      "    # Given a set of reviews (each one a list of words), calculate \n",
      "    # the average feature vector for each one and return a 2D numpy array \n",
      "    # \n",
      "    # Initialize a counter\n",
      "    counter = 0\n",
      "    # \n",
      "    # Preallocate a 2D numpy array, for speed\n",
      "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
      "    # \n",
      "    # Loop through the reviews\n",
      "    for review in reviews:\n",
      "       #\n",
      "       # Print a status message every 1000th review\n",
      "       if counter%1000 == 0:\n",
      "           print \"Review %d of %d\" % (counter, len(reviews))\n",
      "       # \n",
      "       # Call the function (defined above) that makes average feature vectors\n",
      "       reviewFeatureVecs[counter] = makeFeatureVec(review, model, \\\n",
      "           num_features)\n",
      "       #\n",
      "       # Increment the counter\n",
      "       counter = counter + 1\n",
      "    return reviewFeatureVecs"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# ****************************************************************\n",
      "# Calculate average feature vectors for training and testing sets,\n",
      "# using the functions we defined above. Notice that we now use stop word\n",
      "# removal.\n",
      "print \"Creating average feature vecs for training reviews\"\n",
      "clean_train_reviews = []\n",
      "for review in train[\"review\"]:\n",
      "    cleaned_review = review_to_wordlist( review, remove_stopwords=True )\n",
      "    clean_train_reviews.append( removeFurthestCluster(cleaned_review, model) )\n",
      "trainDataVecs = getAvgFeatureVecs( clean_train_reviews, model, num_features )\n",
      "\n",
      "print \"Creating average feature vecs for test reviews\"\n",
      "clean_test_reviews = []\n",
      "for review in test[\"review\"]:\n",
      "    cleaned_review = review_to_wordlist( review, remove_stopwords=True )\n",
      "    clean_test_reviews.append( removeFurthestCluster(cleaned_review, model) )\n",
      "testDataVecs = getAvgFeatureVecs( clean_test_reviews, model, num_features )\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Creating average feature vecs for training reviews\n",
        "Review 0 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 1000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 2000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 3000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 4000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 5000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 6000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 7000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 8000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 9000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 10000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 11000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 12000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 13000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 14000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 15000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 16000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 17000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 18000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 19000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 20000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 21000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 22000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 23000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 24000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Creating average feature vecs for test reviews"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 0 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 1000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 2000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 3000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 4000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 5000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 6000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 7000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 8000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 9000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 10000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 11000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 12000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 13000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 14000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 15000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 16000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 17000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 18000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 19000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 20000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 21000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 22000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 23000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 24000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Prediction 1: Logistic Regression "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn import cross_validation\n",
      "y = train[\"sentiment\"]\n",
      "modelLR = LogisticRegression(penalty='l2', dual=True, tol=0.0001,\n",
      "                         C=1, fit_intercept=True, intercept_scaling=1.0,\n",
      "                         class_weight=None, random_state=None)\n",
      "\n",
      "print \"20 Fold CV Score: \", np.mean(cross_validation.cross_val_score(modelLR, trainDataVecs, y, cv=20, scoring='roc_auc'))\n",
      "print \"Retrain on all training data, predicting test labels...\\n\"\n",
      "\n",
      "modelLR.fit(trainDataVecs,y)\n",
      "result = modelLR.predict_proba(testDataVecs)[:,1]\n",
      "output = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":result} )\n",
      "\n",
      "# Use pandas to write the comma-separated output file\n",
      "output.to_csv( \"Word2Vec_AverageVectors.csv\", index=False, quoting=3 )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "20 Fold CV Score:  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.924932864\n",
        "Retrain on all training data, predicting test labels...\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Prediction 2: Random Forest"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Fit a random forest to the training data, using 100 trees\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "forest = RandomForestClassifier( n_estimators = 100 )\n",
      "\n",
      "print \"20 Fold CV Score: \", np.mean(cross_validation.cross_val_score(forest, trainset, y, cv=20, scoring='roc_auc'))\n",
      "print \"Retrain on all training data, predicting test labels...\\n\"\n",
      "\n",
      "print \"Fitting a random forest to labeled training data...\"\n",
      "forest = forest.fit( trainDataVecs, train[\"sentiment\"] )\n",
      "\n",
      "# Test & extract results \n",
      "result = forest.predict( testDataVecs )\n",
      "\n",
      "# Write the test results \n",
      "output = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":result} )\n",
      "output.to_csv( \"Word2Vec_AverageVectors.csv\", index=False, quoting=3 )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "20 Fold CV Score: "
       ]
      },
      {
       "ename": "NameError",
       "evalue": "name 'trainset' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-8-d67b34821d7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mforest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mn_estimators\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"20 Fold CV Score: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcross_validation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'roc_auc'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Retrain on all training data, predicting test labels...\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mNameError\u001b[0m: name 'trainset' is not defined"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "review = review_to_wordlist( train[\"review\"][0], remove_stopwords=True )\n",
      "removeFurthestClusterTFIDF(review, 0, model, trainset)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(tfv_feature_name_list)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "word_list = []\n",
      "for review in train[\"review\"]:\n",
      "    word_list.append(review_to_wordlist( review, remove_stopwords=True ))\n",
      "for review in unlabeled_train[\"review\"]:\n",
      "    word_list.append(review_to_wordlist( review, remove_stopwords=True ))\n",
      "for review in test[\"review\"]:\n",
      "    word_list.append(review_to_wordlist( review, remove_stopwords=True ))\n",
      "import itertools\n",
      "chain = itertools.chain(*word_list)\n",
      "wl = set(list(chain))\n",
      "len(wl)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "alldata = []\n",
      "for i in xrange( 0, len(train[\"review\"])):\n",
      "    alldata.append(KaggleWord2VecUtility.review_to_wordlist(train[\"review\"][i], False))\n",
      "for i in xrange(0,len(test[\"review\"])):\n",
      "    alldata.append(KaggleWord2VecUtility.review_to_wordlist(test[\"review\"][i], False))\n",
      "for i in xrange(0,len(unlabeled_train[\"review\"])):\n",
      "    alldata.append(KaggleWord2VecUtility.review_to_wordlist(unlabeled_train[\"review\"][i], False))\n",
      "import itertools\n",
      "chain = itertools.chain(*word_list)\n",
      "wl2 = set(list(chain))\n",
      "len(wl2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x=review_to_wordlist( test['review'][22296], remove_stopwords=True )\n",
      "crp(x,model)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wl =[]\n",
      "wl.append(removeFurthestCluster(x, model))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "A=['what','are','you','doing']\n",
      "A = sorted(A)\n",
      "import bisect\n",
      "i=bisect.bisect_left(A, 'you') # last index where value < 50\n",
      "j=bisect.bisect_right(A, 'you') # first index where value > 50\n",
      "print A, i, j"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}