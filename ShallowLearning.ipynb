{
 "metadata": {
  "name": "",
  "signature": "sha256:2df88dddab4de60ded44bfb134086fd2b0bfe3c36706c8dac5c46b454a5d1e72"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Things to do:\n",
      "    1. include unlabeled_train dataset into tfidf vectorizing\n",
      "       Not a improvement. modeling train and test data separately, did not work because size of features does not match\n",
      "    2. sort and select highest tfidf scores words and do word2vec\n",
      "    3. explore more than one gram vectorization"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "import modules and data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "from KaggleWord2VecUtility import KaggleWord2VecUtility\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn import cross_validation\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "train = pd.read_csv('labeledTrainData.tsv', header=0, delimiter=\"\\t\", quoting=3)\n",
      "unlabeled_train = pd.read_csv('unlabeledTrainData.tsv', header=0, delimiter=\"\\t\", quoting=3)\n",
      "test = pd.read_csv('testData.tsv', header=0, delimiter=\"\\t\", quoting=3 )\n",
      "y = train[\"sentiment\"]\n",
      "print \"Cleaning and parsing movie reviews...\\n\"\n",
      "traindata = []\n",
      "for i in xrange( 0, len(train[\"review\"])):\n",
      "    traindata.append(\" \".join(KaggleWord2VecUtility.review_to_wordlist(train[\"review\"][i], True)))\n",
      "testdata = []\n",
      "for i in xrange(0,len(test[\"review\"])):\n",
      "    testdata.append(\" \".join(KaggleWord2VecUtility.review_to_wordlist(test[\"review\"][i], True)))\n",
      "unlabeled_traindata = []\n",
      "for i in xrange(0,len(unlabeled_train[\"review\"])):\n",
      "    unlabeled_traindata.append(\" \".join(KaggleWord2VecUtility.review_to_wordlist(unlabeled_train[\"review\"][i], False)))\n",
      "print 'vectorizing... ', \n",
      "tfv = TfidfVectorizer(min_df=4,  max_features=None, \n",
      "        strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
      "        ngram_range=(1, 4), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
      "        stop_words = None)\n",
      "alldata = traindata + testdata\n",
      "lentrain = len(traindata)\n",
      "lentest = len(testdata)\n",
      "print \"fitting pipeline... \",\n",
      "tfv.fit(alldata)\n",
      "alldata = tfv.transform(alldata)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Cleaning and parsing movie reviews...\n",
        "\n",
        "vectorizing... "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " fitting pipeline... \n"
       ]
      }
     ],
     "prompt_number": 43
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "fit model and make prediction"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "trainset = alldata[:lentrain]\n",
      "testset = alldata[lentrain:]\n",
      "\n",
      "model = LogisticRegression(penalty='l2', dual=True, tol=1e-4,\n",
      "                         C=4.5, fit_intercept=True, intercept_scaling=2.0,\n",
      "                         class_weight=None, random_state=None)\n",
      "print \"20 Fold CV Score: \", np.mean(cross_validation.cross_val_score(model, trainset, y, cv=20, scoring='roc_auc'))\n",
      "\n",
      "print \"Retrain on all training data, predicting test labels...\\n\"\n",
      "model.fit(trainset,y)\n",
      "result = model.predict_proba(testset)[:,1]\n",
      "output = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":result} )\n",
      "\n",
      "# Use pandas to write the comma-separated output file\n",
      "output.to_csv('Bag_of_Words_model.csv', index=False, quoting=3)\n",
      "print \"Wrote results to Bag_of_Words_model.csv\"\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "20 Fold CV Score:  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.965828992\n",
        "Retrain on all training data, predicting test labels...\n",
        "\n",
        "Wrote results to Bag_of_Words_model.csv"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 44
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "alltraindata.shape\n",
      "train.shape\n",
      "testset.getrow(0).shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 36,
       "text": [
        "(1, 47071)"
       ]
      }
     ],
     "prompt_number": 36
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "0.965830144"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 35,
       "text": [
        "array([46810, 45079, 44985, 43191, 41914, 41536, 41207, 40809, 40453,\n",
        "       40235, 40033, 39748, 38428, 37660, 37630, 37594, 36628, 35828,\n",
        "       34243, 33670, 33550, 31641, 31563, 30708, 29887, 29837, 29229,\n",
        "       29166, 28678, 28167, 28133, 28057, 28053, 27421, 25813, 25266,\n",
        "       25227, 24698, 23973, 22428, 21427, 21188, 20772, 20146, 19306,\n",
        "       15708, 15468, 15297, 14985, 13586, 13543, 13366, 11685, 11120,\n",
        "        9192,  8529,  8107,  6670,  6621,  3390,  1419], dtype=int32)"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "first = zip(alldata.getrow(140).indices, alldata.getrow(140).data)\n",
      "first.sort(key=lambda x: x[1], reverse = True)\n",
      "for i in xrange(40):\n",
      "    print tfv.get_feature_names()[first[i][0]]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "birth\n",
        "west"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "incite"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "branding"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "fluids"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "calf"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "hardship"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "class"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "pigs"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "slaughtered"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "keeper"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "vomit"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "farmer"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "late"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "drove"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "live"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "widow"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "bathroom"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "cowboy"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "disgusting"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "life"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "numerous"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "accurate"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "saw"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "purpose"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "choice"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "premise"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "view"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "daughter"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "ok"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "history"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "wanted"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "oh"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "human"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "house"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "tell"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "came"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "story"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "shot"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "american"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}